{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a8d16d4864a6415fb79e362360714568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a790c84edb8476587c043d4622fc75b",
              "IPY_MODEL_c767905f108b4cc2aa09e6732afa7cbd",
              "IPY_MODEL_68ae6d3be9e346cb9efa4c16977d634c"
            ],
            "layout": "IPY_MODEL_56eca406cfd54cf68f2689445c560dbe"
          }
        },
        "1a790c84edb8476587c043d4622fc75b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_240de72e8cc9430e9e3017d04f56a2d7",
            "placeholder": "​",
            "style": "IPY_MODEL_cb146d0dbbfa44f3bbc09dca96c26455",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: "
          }
        },
        "c767905f108b4cc2aa09e6732afa7cbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2299f25fc23e43f3b6c46924b91e8cf0",
            "max": 28918,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91275180987e4edaa57a6699e4607e43",
            "value": 28918
          }
        },
        "68ae6d3be9e346cb9efa4c16977d634c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c49cc4c5a298443e996ca84c8fb19ca4",
            "placeholder": "​",
            "style": "IPY_MODEL_415d2281247f49d5a4dda88dea7ff8bd",
            "value": " 193k/? [00:00&lt;00:00, 9.17MB/s]"
          }
        },
        "56eca406cfd54cf68f2689445c560dbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "240de72e8cc9430e9e3017d04f56a2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb146d0dbbfa44f3bbc09dca96c26455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2299f25fc23e43f3b6c46924b91e8cf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91275180987e4edaa57a6699e4607e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c49cc4c5a298443e996ca84c8fb19ca4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "415d2281247f49d5a4dda88dea7ff8bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NER\n",
        "\n",
        "In this Notebook the parsed datasets are used to run the NER models and compute the performance metrics."
      ],
      "metadata": {
        "id": "jwyn_B-LdDg8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgzI_-rScARB",
        "outputId": "04cb6c06-6ef0-4c10-eade-4bfc49dce86a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.10.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.22.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.8/dist-packages (1.4.2)\n",
            "Requirement already satisfied: classla in /usr/local/lib/python3.8/dist-packages (1.1.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (4.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (2.11.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (14.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (0.28.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (22.12.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (1.51.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (3.19.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (2.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (21.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (2.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.38.4)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (2.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow-gpu) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-gpu) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-gpu) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-gpu) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-gpu) (2.0.12)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow-gpu) (3.0.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.8/dist-packages (from stanza) (2.2.0)\n",
            "Requirement already satisfied: obeliks==1.1.5 in /usr/local/lib/python3.8/dist-packages (from classla) (1.1.5)\n",
            "Requirement already satisfied: reldi-tokeniser==1.0.1 in /usr/local/lib/python3.8/dist-packages (from classla) (1.0.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from obeliks==1.1.5->classla) (4.9.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-gpu torch pandas numpy scikit-learn transformers spacy stanza classla nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## I/O device registering\n",
        "\n",
        "Current working directory is set to /content by default. You can also give access to your Google Drive to save models/results/... there.\n"
      ],
      "metadata": {
        "id": "m-CcLhBRd7cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "# Access your Drive data using folder '/content/drive/MyDrive'\n",
        "\n",
        "# Set the working directory\n",
        "workdir = \"/content/drive/MyDrive/ml_ner/datasets\"\n",
        "\n",
        "!ls -lah \"$workdir\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvVUVPBNd23O",
        "outputId": "9c7313cd-cea9-440e-c816-b7b48e404915"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "total 2.5M\n",
            "drwx------ 2 root root 4.0K Jan  2 19:23 btc\n",
            "drwx------ 2 root root 4.0K Dec 30 19:28 CoNLL03\n",
            "drwx------ 2 root root 4.0K Jan  1 18:43 emtd\n",
            "-rw------- 1 root root 2.4M Dec 30 19:30 entity-recognition-datasets-master.zip\n",
            "drwx------ 2 root root 4.0K Jan  2 19:23 wikigold\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File management\n",
        "\n",
        "Create the results folder under datasets.\n",
        "\n",
        "Set the results path.\n",
        "\n",
        "**Both datasets are licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).**"
      ],
      "metadata": {
        "id": "ff4re69PyB7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the datasets results directories\n",
        "!mkdir -p \"$workdir\"'/wikigold/results' \"$workdir\"'/btc/results'"
      ],
      "metadata": {
        "id": "vRqRDnrq2L9s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read in and parse\n",
        "\n",
        "Read in all the annotated data and parse it into a unified format for evaluation."
      ],
      "metadata": {
        "id": "Dpnc3Xmc53-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "from pathlib import PurePath"
      ],
      "metadata": {
        "id": "RD4nDf1cHl36"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the data\n",
        "def decode_data(file):\n",
        "  # Read the file\n",
        "  with open(file) as f:\n",
        "    dataset = json.loads(f.read())\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "# Write out the data\n",
        "def encode_data(path, filename, output):\n",
        "  # Export the json file\n",
        "  json_dump = json.dumps(output)\n",
        "\n",
        "  with open(PurePath(path, filename), \"w\") as f:\n",
        "    f.write(json_dump)"
      ],
      "metadata": {
        "id": "nqwLmVrxSYFl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check GPU resources"
      ],
      "metadata": {
        "id": "qcuam98eeNpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQYW7peleQAU",
        "outputId": "aa692f38-1296-440c-b0f5-65d1d822b8af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan  2 19:48:44 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P0    26W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check TensorFlow compatibilty"
      ],
      "metadata": {
        "id": "A2OBJwv2eWGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "print(f\"Tensorflow version: {tf.__version__}\")\n",
        "\n",
        "# Restrict TensorFlow to only allocate 4GBs of memory on the first GPU\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(f\"The system contains '{len(gpus)}' Physical GPUs and '{len(logical_gpus)}' Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    print(e)\n",
        "else:\n",
        "    print(f\"Your system does not contain a GPU that could be used by Tensorflow!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsZ1D1oQeevW",
        "outputId": "c123b289-148e-4d81-e830-218c1348d30e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version: 2.11.0\n",
            "The system contains '1' Physical GPUs and '1' Logical GPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER pre-trained model wrappers\n",
        "\n",
        "Bellow are the functions that emplore the NER methods available in spaCy, Stanza, Classla and NLTK NER systems.\n",
        "\n",
        "Each system is also initialized and prepared to process sentences."
      ],
      "metadata": {
        "id": "Og12AUuUdB7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### spaCy\n",
        "\n",
        "Model: `en_core_web_trf`.\n",
        "\n",
        "Format: IOB"
      ],
      "metadata": {
        "id": "L7diKdGzfLoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model\n",
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff91XpWfd3dt",
        "outputId": "f6ed31fe-6408-45b3-c45a-49162a4ca7fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-01-02 19:49:03.313781: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-01-02 19:49:03.313913: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-01-02 19:49:03.313936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-trf==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.4.1/en_core_web_trf-3.4.1-py3-none-any.whl (460.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 460.3 MB 27 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.1 in /usr/local/lib/python3.8/dist-packages (from en-core-web-trf==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: spacy-transformers<1.2.0,>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from en-core-web-trf==3.4.1) (1.1.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (4.62.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (21.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (1.22.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (2.10)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.8/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.1) (0.9.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.1) (1.10.1)\n",
            "Requirement already satisfied: transformers<4.26.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.1) (4.25.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<4.26.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.1) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers<4.26.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.1) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<4.26.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.1) (3.8.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers<4.26.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.1) (0.11.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<4.26.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.4.1) (0.13.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.1->en-core-web-trf==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import spacy\n",
        "\n",
        "# Load the model\n",
        "nlp_spacy = spacy.load(\"en_core_web_trf\")"
      ],
      "metadata": {
        "id": "ID06up6KfV_C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to call\n",
        "def ner_spacy(sentence):\n",
        "  # Get the global variable\n",
        "  global nlp_spacy\n",
        "\n",
        "  # Process the sentence\n",
        "  doc = nlp_spacy(sentence)\n",
        "\n",
        "  # Return list of IOB entities\n",
        "  return [token.ent_iob_ if not token.ent_type_ else f\"{token.ent_iob_}-{token.ent_type_}\" for token in doc]"
      ],
      "metadata": {
        "id": "NCgvdvz2fhew"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for correct output\n",
        "print(ner_spacy(\"010 is the tenth album from Japanese Punk Techno band The Mad Capsule Markets.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5mmVgA4p4bb",
        "outputId": "1cce223d-f0ed-4b4f-edf0-941b986dd92c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['B-CARDINAL', 'O', 'O', 'B-ORDINAL', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stanza\n",
        "\n",
        "Model: `en`.\n",
        "\n",
        "Format: BIOES"
      ],
      "metadata": {
        "id": "y-sVbem3gMiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import stanza\n",
        "\n",
        "# Initialize the pipeline\n",
        "nlp_stanza = stanza.Pipeline(lang = \"en\", processors = \"tokenize,ner\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "a8d16d4864a6415fb79e362360714568",
            "1a790c84edb8476587c043d4622fc75b",
            "c767905f108b4cc2aa09e6732afa7cbd",
            "68ae6d3be9e346cb9efa4c16977d634c",
            "56eca406cfd54cf68f2689445c560dbe",
            "240de72e8cc9430e9e3017d04f56a2d7",
            "cb146d0dbbfa44f3bbc09dca96c26455",
            "2299f25fc23e43f3b6c46924b91e8cf0",
            "91275180987e4edaa57a6699e4607e43",
            "c49cc4c5a298443e996ca84c8fb19ca4",
            "415d2281247f49d5a4dda88dea7ff8bd"
          ]
        },
        "id": "sxCB1OKwhAb5",
        "outputId": "856e14c5-34b4-4891-83a9-6aa2f9e0598d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8d16d4864a6415fb79e362360714568"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | combined  |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "INFO:stanza:Use device: gpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to call\n",
        "def ner_stanza(sentence):\n",
        "  # Get the global variable\n",
        "  global nlp_stanza\n",
        "\n",
        "  # Process the sentence\n",
        "  doc = nlp_stanza(sentence)\n",
        "\n",
        "  # Return a list of BIOES entities\n",
        "  return [token.ner for sent in doc.sentences for token in sent.tokens]"
      ],
      "metadata": {
        "id": "_Jv1JQXEhvme"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for correct output\n",
        "print(ner_stanza(\"010 is the tenth album from Japanese Punk Techno band The Mad Capsule Markets.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a9ca87-96e6-45b7-9003-074e51090ac3",
        "id": "k-y5Ap4bwA21"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['S-CARDINAL', 'O', 'O', 'S-ORDINAL', 'O', 'O', 'S-NORP', 'B-ORG', 'E-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'E-ORG', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classla\n",
        "\n",
        "Model: `sl`.\n",
        "\n",
        "Format: BIOES"
      ],
      "metadata": {
        "id": "Pg-47Wk_iHKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import classla\n",
        "\n",
        "# Download the model\n",
        "classla.download('sl')\n",
        "\n",
        "# Initialize the pipeline\n",
        "nlp_classla = classla.Pipeline(lang = \"sl\", processors = \"tokenize,ner\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VC9KxrAiClH",
        "outputId": "66ebb562-76c6-49a9-b432-9d3e7078dafc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://raw.githubusercontent.com/clarinsi/classla-resources/main/resources_1.0.1.json: 10.3kB [00:00, 2.34MB/s]                   \n",
            "INFO:classla:Downloading these customized packages for language: sl (Slovenian)...\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | standard |\n",
            "| pos       | standard |\n",
            "| lemma     | standard |\n",
            "| depparse  | standard |\n",
            "| ner       | standard |\n",
            "| pretrain  | standard |\n",
            "========================\n",
            "\n",
            "INFO:classla:File exists: /root/classla_resources/sl/pos/standard.pt.\n",
            "INFO:classla:File exists: /root/classla_resources/sl/lemma/standard.pt.\n",
            "INFO:classla:File exists: /root/classla_resources/sl/depparse/standard.pt.\n",
            "INFO:classla:File exists: /root/classla_resources/sl/ner/standard.pt.\n",
            "INFO:classla:File exists: /root/classla_resources/sl/pretrain/standard.pt.\n",
            "INFO:classla:Finished downloading models and saved to /root/classla_resources.\n",
            "INFO:classla:Loading these models for language: sl (Slovenian):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | standard |\n",
            "| ner       | standard |\n",
            "========================\n",
            "\n",
            "INFO:classla:Use device: gpu\n",
            "INFO:classla:Loading: tokenize\n",
            "INFO:classla:Loading: ner\n",
            "INFO:classla:Done loading processors!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to call\n",
        "def ner_classla(sentence):\n",
        "  # Get the global variable\n",
        "  global nlp_classla\n",
        "\n",
        "  # Process the sentence\n",
        "  doc = nlp_classla(sentence)\n",
        "\n",
        "  # Return a list of BIOES entities\n",
        "  return [token.ner for sent in doc.sentences for token in sent.tokens]"
      ],
      "metadata": {
        "id": "HO0P-HY2wbqi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for correct output\n",
        "print(ner_classla(\"010 is the tenth album from Japanese Punk Techno band The Mad Capsule Markets.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b660019b-268b-4688-8274-a207a0cf7f77",
        "id": "dq6BR5Fvwbql"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK\n",
        "\n",
        "Model: `built in`.\n",
        "\n",
        "Format: IOB"
      ],
      "metadata": {
        "id": "mmnJynHCidR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES22fnIVidR5",
        "outputId": "8627d6af-6174-45b1-d819-8b9eb566bcd1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to call\n",
        "def ner_nltk(sentence):\n",
        "  # Process the sentence\n",
        "\n",
        "  # Tokenization\n",
        "  tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "  # Tagging\n",
        "  tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "  # NER\n",
        "  entities = nltk.chunk.ne_chunk(tagged_tokens)\n",
        "\n",
        "  # Transform tree to conll tags\n",
        "  conll_tags = nltk.chunk.tree2conlltags(entities)\n",
        "\n",
        "  # Return a list of IOB entities\n",
        "  return [conll_tag[2] for conll_tag in conll_tags]"
      ],
      "metadata": {
        "id": "VzNr9HS2idR6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test for correct output\n",
        "print(ner_nltk(\"010 is the tenth album from Japanese Punk Techno band The Mad Capsule Markets.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSVtHjt10ORv",
        "outputId": "08117da1-3b44-468e-93fb-1104f53ad9be"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main execution\n",
        "\n",
        "Run the NER taggers over the prepared sentances in the dataset and store the resulting entities/attributes for further analysis."
      ],
      "metadata": {
        "id": "UXbu6nly4LGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of wrappers to execute\n",
        "ner_wrappers = [ner_spacy, ner_stanza, ner_classla, ner_nltk]\n",
        "result_keys = [\"spacy_entities\", \"stanza_entities\", \"classla_entities\", \"nltk_entities\"]\n",
        "\n",
        "# Set filepaths for read in\n",
        "filepaths = [PurePath(workdir, \"wikigold\"), PurePath(workdir, \"btc\")]\n",
        "\n",
        "# Process all the files\n",
        "for filepath in filepaths:\n",
        "  for path, _, files in os.walk(PurePath(filepath, \"parsed\")):\n",
        "    for name in files:\n",
        "      # Compile the absolute filepath\n",
        "      file = PurePath(path, name)\n",
        "\n",
        "      # Read in the dataset\n",
        "      dataset = decode_data(file)\n",
        "\n",
        "      # Iterate over the sentences in the dataset\n",
        "      for entry in dataset:\n",
        "        # Run all the NERs and save the results\n",
        "        for ner_wrapper, result_key in zip(ner_wrappers, result_keys):\n",
        "          entry[result_key] = ner_wrapper(entry[\"sentence\"])\n",
        "\n",
        "      # Get the filename and save the data\n",
        "      filename = f\"{file.stem}_results.json\"\n",
        "\n",
        "      encode_data(PurePath(filepath, \"results\"), filename, dataset)\n"
      ],
      "metadata": {
        "id": "uU20uHLP6rPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85557eb8-a8d8-4cbb-ad0f-92b5b386df09"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Fresh sheets&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Check out these photos&gt;&gt; http://t.co/3FaiJNoi Looking for a venue for staff parties/birthday/... #birthday #parties #privatehire\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Check out these photos&gt;&gt; http://t.co/3FaiJNoi Looking for a venue for staff parties/birthday/... #birthday #parties #privatehire\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT WhosYungNikey: SheCoolHuh ahahha sonu you just clever as hell xD&lt; Tell me something I do n't know please!\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT SophieTweddle: jernade yooooow x&lt; whts guuuud\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Ca n't help but think of the times I 've had with you, pictures and some memories will have to help me through. #nowplaying Dear God- A 7x&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"'\"' DaBlackMiniMe: If Someone Wants To Be In Your Life They Will Make Extra Effort To Do So '\"'&lt; that 's my moto!\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Spend at most 10 minutes analysing these letters&gt;&gt;&gt; OGC&lt;&lt;&lt; hehehehehe\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Spend at most 10 minutes analysing these letters&gt;&gt;&gt; OGC&lt;&lt;&lt; hehehehehe\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Spend at most 10 minutes analysing these letters&gt;&gt;&gt; OGC&lt;&lt;&lt; hehehehehe\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Spend at most 10 minutes analysing these letters&gt;&gt;&gt; OGC&lt;&lt;&lt; hehehehehe\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Spend at most 10 minutes analysing these letters&gt;&gt;&gt; OGC&lt;&lt;&lt; hehehehehe\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Spend at most 10 minutes analysing these letters&gt;&gt;&gt; OGC&lt;&lt;&lt; hehehehehe\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Check this video out--Machine Gun Kelly Official Label Announcement http://t.co/TTKSH79C via youtube&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"RT mmrelish: welovemalton A perfect reason to pop into Malton today&gt;&gt;&gt; Today 's special-giant yorkshires and beef http://t.co/6nPWOldu\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"RT mmrelish: welovemalton A perfect reason to pop into Malton today&gt;&gt;&gt; Today 's special-giant yorkshires and beef http://t.co/6nPWOldu\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"RT mmrelish: welovemalton A perfect reason to pop into Malton today&gt;&gt;&gt; Today 's special-giant yorkshires and beef http://t.co/6nPWOldu\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Mood dramatically improved by trying on formal dress again&lt; 3 love it #excited. com\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Oke. Alright. Fine. Umma. Jahat RT _riichan umma seneng ngeliatin appa galau x)/plak. bunu sana&gt; D/provokator... http://t.co/AYiPJ1ze\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Added a new video:&quot; Effy&lt; U. S. Edit&gt;&quot; http://t.co/lzCRy4Tk&#35; video\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Added a new video:&quot; Effy&lt; U. S. Edit&gt;&quot; http://t.co/lzCRy4Tk&#35; video\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"One of if not the most chilled instrumental ever. I know LennyHirsch shares the same love for ultimate chill. http://t.co/Y10UGEDS&lt; 3&lt; 3&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"One of if not the most chilled instrumental ever. I know LennyHirsch shares the same love for ultimate chill. http://t.co/Y10UGEDS&lt; 3&lt; 3&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"One of if not the most chilled instrumental ever. I know LennyHirsch shares the same love for ultimate chill. http://t.co/Y10UGEDS&lt; 3&lt; 3&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT HachyHflava: RT ckpthamayor: After reading about Rihanna 's thug life tattoo, I think we all owe Chris Brown an apology.&lt; Loooool\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"RT dave_ashworth: BEST HEADLINE EVER: Gordon Ramsay 's Dwarf Porn Double Found Dead in a Badger Den in Wales-&gt; http://t.co/1xfAVkYq\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT CreamYaSkin: Babestation charge ridiculous prices&lt; stupid prices for rubbish\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT roblitawski: It 's finally arrived! The iPhone is safe! http://t.co/qeo2bOt2&lt;&lt; 2 weeks MAX\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT roblitawski: It 's finally arrived! The iPhone is safe! http://t.co/qeo2bOt2&lt;&lt; 2 weeks MAX\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT XianLoves: DeejayAD_0121 I need to come to your next booking. Holla&lt; jeztadj B 'day bash in Feb\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Good vintage clothing advice by stylish ginamaldina right here--&gt; http://t.co/n8bEl90O\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"#IfThereWasntAnyPolice&lt; surely that should be werent? '\"' if there were n't any police '\"' yeah.\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"&lt; listening&gt; Metallica: Battery&lt; /listening&gt;\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"&lt; listening&gt; Metallica: Battery&lt; /listening&gt;\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"&lt; listening&gt; Metallica: Battery&lt; /listening&gt;\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"&lt; listening&gt; Metallica: Battery&lt; /listening&gt;\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@Yuri_Smith:@Reece_Webbz I second that fam, let 's gettit!&lt; Not good mayne, back on it tho!\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Sleep, i 'm actually tired for once&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Alarm Clock&gt; Power Tools\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Did replacing your BT Infinity Homehub improve #broadband? Swap out of these Huawei devices underway for new model (though mine&lt; 1 yr old)\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"“@TheRealMacLovin: RT@Wendy_Dimpz: RT@Han_Hanzz: RT@ElHefeJones: #InAnAfricanHouse koka oats. I fort it was jus me! Loooooool”&lt; oooh woow\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Preach brother --&gt; “@ZachWehner: Let me say something that will offend a lot of people before I go to sleep.. The Weeknd is TRASH. Goodnight\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@holliebooby: Loool@the person who just met Bevis today.&lt; -____-\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@AngelaSimmons: NYC Sky! http://t.co/1APGFcFc&lt;&lt; beeeauriful\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@AngelaSimmons: NYC Sky! http://t.co/1APGFcFc&lt;&lt; beeeauriful\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"“@Mervy_Merv: My avi is n't changing: (”&lt;&lt; u tryna follow the advice if the creep that DM'ed you*?\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"“@Mervy_Merv: My avi is n't changing: (”&lt;&lt; u tryna follow the advice if the creep that DM'ed you*?\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Belfast ---&gt; London. #mindthegap\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@TalkShitBitches:@jernade you 're still up?: O i think you should follow me, seen as i love you!&lt; 3&lt; safe. done\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@TalkShitBitches:@jernade you 're still up?: O i think you should follow me, seen as i love you!&lt; 3&lt; safe. done\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"El verdadero amor perdonano abandona, no se quiebrano aprisiona, no revientacomo pompas de jabón.&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@milkshake11s:@yeovilpeople will look forward to seeing #ytfc move up!!!! With #Garyjohnson as the new manager&lt; -- hope so!\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Also: I would like to pay homage to Office for Mac 2011 ... a real lifeline today.&lt; sighs in relief&gt;\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Also: I would like to pay homage to Office for Mac 2011 ... a real lifeline today.&lt; sighs in relief&gt;\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Was listening to the #bionic album this morning! Can not wait for #LoveYourBodyXtina&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Oh my god, Better Than I Know Myself by@adamlambert up loud on headphones&gt;&gt;&gt;&gt;&gt;&gt;\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Oh my god, Better Than I Know Myself by@adamlambert up loud on headphones&gt;&gt;&gt;&gt;&gt;&gt;\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Oh my god, Better Than I Know Myself by@adamlambert up loud on headphones&gt;&gt;&gt;&gt;&gt;&gt;\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Oh my god, Better Than I Know Myself by@adamlambert up loud on headphones&gt;&gt;&gt;&gt;&gt;&gt;\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Oh my god, Better Than I Know Myself by@adamlambert up loud on headphones&gt;&gt;&gt;&gt;&gt;&gt;\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"Oh my god, Better Than I Know Myself by@adamlambert up loud on headphones&gt;&gt;&gt;&gt;&gt;&gt;\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"RT@IntegratedEA: Are you attending Integrated EA 2012? Let us know http://t.co/o6pMhCU7 #Entarch #EA&gt; recommended EA conference\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"After today. I 've learned that some people can fuck off because the good news keeps on coming: D&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"just had a cuddle with my pup&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"RT@charleslavery DISGRACE! Student Hacker Can Be Extradited To The US. http://t.co/ZnNWS74C&gt; Craven obeisance to a truly #EvilEmpire\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"#ff@blitzkids@nicola_craig@NicolaaaaBLITZ@lornblitz@FionaBlitz@i_jumelle@jackgoodmanmate@whoisKATY@yasminBLITZ@ellabtwjackson&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"#FF@ecstacey_@jennieefye@imarapeyoface@LetsGetUglyy@LittleBlueBoob@patsyyyyyy&lt; 3 :3\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Crew Love cover ... http://t.co/GZrMWz3L&lt; ---He sings there 's a room full of ninjas, what you following me for ... EPIC!\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"RT@Thisisglos: Congratulations to@TheocHouse for winning a #Camra award so quickly http://t.co/XNKfJn5q&gt; great place\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Nice run out at roko now for some LJ 's cusine #i&lt; 3LJ 'S\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"“@Nikeeetz: On my way back to London.”&lt; do n't think anyone actually cares tbh\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Believe Tour To Belfast&lt; 3 #40\"\n",
            "Warning: Cannot compute token index. Token: \"&gt;\" Text: \"#YouKnowYoureDrunkWhen you finish being a Pirate Princess Pickle Bunny like this guy --&gt; http://t.co/IrgL2Cgu\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@mikeparry8: Rooney 's limping around. Worrying.A greater England player than Phil Jones. I do n't care about you critics.&lt;&lt; you get worse\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@mikeparry8: Rooney 's limping around. Worrying.A greater England player than Phil Jones. I do n't care about you critics.&lt;&lt; you get worse\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@stedavies: Publicasity PR only taking Oxbridge grads b/c they want 'thinkers 'http://t.co/9zZJMvmn&lt; -- embarrassing.\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Boris Johnson lined up for safe Tory seat ahead of 2015 general election http://t.co/bxB0OLi7 via@guardian&lt; so he can succeed Cameron!\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@SupanovaDex: Happy Birthday to my bro@CyrilleSP&lt; much love Dex!! ... how 's things going?\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"SHOUTOUT TO Louise and Annabelle?:) PreTTy Please:)&lt; 3 #TwentyTwentyTwitcam&lt; 3 (@ttband live on http://t.co/1PX0RwSP)\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"SHOUTOUT TO Louise and Annabelle?:) PreTTy Please:)&lt; 3 #TwentyTwentyTwitcam&lt; 3 (@ttband live on http://t.co/1PX0RwSP)\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Apparently I looked good tonight. That was lovely to head. #sexygingermenarethebest.&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"“@Chanbrowninx: Who told me to eat Jerk chicken in the morning”&lt; ~| warrior\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@LittleMissVak:@JoannaWise1@mrbozie just got@Danny_reload for next week at this sugar hut gaff.&lt; -- u wan na be clickin attendingggg\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@JiggzyOnline: Man ah informah. wen mi see ah man shot food n mek a likkle money, Mi ah inform da poliiiice.&lt; *gunshot fi di informah*\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Got some great shows to anounce to you very soon! Get excited, cus we are!:)&lt; 3 x\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"“@Devante_C: If a gay guy offered you 100K to suck your dick ... Would you accept? ”&lt; potentially! LOL\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Good Morning Everyone!&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"Jono from hollyoaks is waaaaaaaaaaaaaaaaaay to perfect&lt; 3\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@Oloni:@Zailaniii_xox:@DammyVenables and@Oloni: p http://t.co/expHxnJJ&lt; LOL *blushes* • *hugs*;)\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"#BexApproved&lt; 3 Adore http://t.co/sgf8ChlX\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"@xMIZZWILSONx: Mmm love green tea so much ... another cup already!&lt;&lt;&lt; Ginger&lemon is the one for me! 8-)\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"@xMIZZWILSONx: Mmm love green tea so much ... another cup already!&lt;&lt;&lt; Ginger&lemon is the one for me! 8-)\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"@xMIZZWILSONx: Mmm love green tea so much ... another cup already!&lt;&lt;&lt; Ginger&lemon is the one for me! 8-)\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@RealJustJames: RT@KatyColeWorld: LOOOL so guys are wearing leggings now? http://t.co/5bIHIMwQ&lt;@MasterDeeA&lt; FFS why!\"\n",
            "Warning: Cannot compute token index. Token: \"&lt;\" Text: \"RT@RealJustJames: RT@KatyColeWorld: LOOOL so guys are wearing leggings now? http://t.co/5bIHIMwQ&lt;@MasterDeeA&lt; FFS why!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flush changes\n",
        "\n",
        "If ran in Google Colaboratory"
      ],
      "metadata": {
        "id": "bWA1kiODegl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this at the end\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jsjpf_jSF9L",
        "outputId": "fa406110-3c50-47b0-d6eb-ec60d8748381"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All changes made in this colab session should now be visible in Drive.\n"
          ]
        }
      ]
    }
  ]
}