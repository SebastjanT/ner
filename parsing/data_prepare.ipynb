{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook for preparing the data for further processing\n",
        "\n",
        "## Preparing the NER evaluation datasets\n",
        "\n",
        "First download the zip of the [repository](https://github.com/juand-r/entity-recognition-datasets) available on Github.\n",
        "\n",
        "You can also clone the repository, but this Notebook is geared towards unpacking the required datasets from a `.zip` file."
      ],
      "metadata": {
        "id": "9t_jbUr0_YW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the filename variable (without suffix)\n",
        "filename = \"entity-recognition-datasets-master\""
      ],
      "metadata": {
        "id": "C5b0EATdzJUi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working directory\n",
        "\n",
        "This Notebook is design to run in Google Colaboratory and as such the working directory is a folder inside Google Drive.\n",
        "\n",
        "If you wish to use a local IPython instance make sure to update the working directory and leave out the step of connectind the Notebook to Google Drive."
      ],
      "metadata": {
        "id": "xZqN_IHu__1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "# Access your Drive data using folder '/content/drive/MyDrive'\n",
        "\n",
        "# Set the working directory\n",
        "workdir = \"/content/drive/MyDrive/ml_ner/datasets\"\n",
        "\n",
        "!ls -lah \"$workdir\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7xBMC2cAfZ4",
        "outputId": "80847165-14e0-40bd-b7d4-8fd097928fff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "total 2.5M\n",
            "drwx------ 2 root root 4.0K Dec 30 19:28 CoNLL03\n",
            "drwx------ 2 root root 4.0K Jan  1 18:43 emtd\n",
            "-rw------- 1 root root 2.4M Dec 30 19:30 entity-recognition-datasets-master.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File management\n",
        "\n",
        "Unzip the copy of the repo and copy out the selected datasets (Wikigold and BTC). Remove the rest and the extracted repository files.\n",
        "\n",
        "**Both datasets are licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).**"
      ],
      "metadata": {
        "id": "ff4re69PyB7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the files\n",
        "!unzip \"$workdir/$filename\"'.zip' -d \"$workdir\""
      ],
      "metadata": {
        "id": "q6vAiDoWyA0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataset directories and copy the dataset files\n",
        "!mkdir -p \"$workdir\"'/wikigold/parsed' \"$workdir\"'/btc/parsed'\n",
        "\n",
        "!cp \"$workdir/$filename\"'/data/wikigold/CONLL-format/data/wikigold.conll.txt' \"$workdir\"'/wikigold'\n",
        "!cp \"$workdir/$filename\"'/data/BTC/CONLL-format/data/'*'.conll' \"$workdir\"'/btc'\n",
        "\n",
        "# Remove the extracted repository files\n",
        "!rm -r \"$workdir/$filename\""
      ],
      "metadata": {
        "id": "vRqRDnrq2L9s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read in and parse\n",
        "\n",
        "Read in all the annotated data and parse it into a unified format for evaluation."
      ],
      "metadata": {
        "id": "Dpnc3Xmc53-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "from pathlib import PurePath"
      ],
      "metadata": {
        "id": "RD4nDf1cHl36"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for parsin the data in dataset files\n",
        "def parse_data(fp):\n",
        "  # List of dicts for data {sentence, words, NE annotations}\n",
        "  dataset = []\n",
        "\n",
        "  # Iteration variables\n",
        "  sentence = \"\"\n",
        "  words = []\n",
        "  annotations = []\n",
        "\n",
        "  # Increment line by line\n",
        "  for line in fp:\n",
        "    # Line is empty (new sentence)\n",
        "    if line in [\"\\n\", \"\\r\\n\"]:\n",
        "      # Write sentence to dataset in specified dict format\n",
        "      dataset.append({\n",
        "          \"sentence\": sentence.rstrip(),\n",
        "          \"words\": words,\n",
        "          \"annotations\": annotations\n",
        "      })\n",
        "\n",
        "      # Clear the iteration variables\n",
        "      sentence = \"\"\n",
        "      words = []\n",
        "      annotations = []\n",
        "\n",
        "      continue\n",
        "\n",
        "    # Exclusions (special cases that need to be filtered out)\n",
        "    if line in [\"-DOCSTART- 0\"]:\n",
        "      continue\n",
        "\n",
        "    # Somehow the dataset contains whitespaces in the classification (removing)\n",
        "    if len(line.split()) != 2:\n",
        "      continue\n",
        "\n",
        "    # Build the sentence and add the word with annotation to the current lists\n",
        "    word, annotation = line.split()\n",
        "    words.append(word)\n",
        "    annotations.append(annotation)\n",
        "\n",
        "    # If word is special character remove whitespaces before building.\n",
        "    rule_leading = re.compile('[,.!?;:\\-_@%^&*)}=+/?\\\\|]')\n",
        "    rule_trailing = re.compile('[<>\"\\'\\-_@#$^&*({=+/\\\\|]')\n",
        "\n",
        "    if len(word) == 1:\n",
        "      if rule_leading.match(word):\n",
        "        sentence = sentence.rstrip()\n",
        "  \n",
        "      # Add word to sentence without whitespace\n",
        "      if rule_trailing.match(word):\n",
        "        sentence += word\n",
        "        continue\n",
        "\n",
        "    # Add word+trailing whitespace to sentence\n",
        "    sentence += f\"{word} \"\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "tpEI7pqQ80zF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write out the data\n",
        "def encode_data(path, filename, output):\n",
        "  # Export the json file\n",
        "  json_dump = json.dumps(output)\n",
        "\n",
        "  with open(PurePath(path, filename), \"w\") as f:\n",
        "    f.write(json_dump)"
      ],
      "metadata": {
        "id": "nqwLmVrxSYFl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set filepaths for read and parse\n",
        "filepaths = [PurePath(workdir, \"wikigold\"), PurePath(workdir, \"btc\")]\n",
        "\n",
        "# Process all the files\n",
        "for filepath in filepaths:\n",
        "  for path, _, files in os.walk(filepath):\n",
        "    for name in files:\n",
        "      # Skip subfolders\n",
        "      if PurePath(path) != filepath:\n",
        "        continue\n",
        "      # Compile the absolute filepath\n",
        "      file = PurePath(filepath, name)\n",
        "\n",
        "      # Open the file\n",
        "      with open(file) as fp:\n",
        "        # Process the file\n",
        "        dataset = parse_data(fp)\n",
        "\n",
        "        # Get filename\n",
        "        filename = f\"{name.split('.')[0]}.json\"\n",
        "        \n",
        "        # Write the parsed data to json file\n",
        "        encode_data(PurePath(filepath, \"parsed\"), filename, dataset)\n"
      ],
      "metadata": {
        "id": "uU20uHLP6rPi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flush changes\n",
        "\n",
        "If ran in Google Colaboratory"
      ],
      "metadata": {
        "id": "lfVLobSRSIVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this at the end\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "print('All changes made in this colab session should now be visible in Drive.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZth3rb0SKFF",
        "outputId": "034fbacb-f777-4fcf-b17f-6f05931c0258"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All changes made in this colab session should now be visible in Drive.\n"
          ]
        }
      ]
    }
  ]
}